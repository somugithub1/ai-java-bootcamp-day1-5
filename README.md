# AI Backend Platform (Spring Boot + Ollama + RAG)

A production-ready AI backend built with Spring Boot that supports:

-   Local LLM inference via Ollama
-   Embeddings generation
-   Vector search abstraction
-   Retrieval-Augmented Generation (RAG)
-   Evaluation layer
-   Observability hooks
-   Clean LLM provider strategy pattern

------------------------------------------------------------------------

## ğŸš€ Architecture Overview

``` mermaid
flowchart TD
    A[Client] --> B[ChatController]
    B --> C[ChatService]
    C --> D[PromptBuilder]
    C --> E[RagService]
    E --> F[EmbeddingService]
    F --> G[VectorStore]
    E --> H[LlmProvider]
    H --> I[OllamaProvider]
    G --> E
```

### Component Responsibilities

-   **Controller Layer**: Exposes REST APIs.
-   **Service Layer**: Handles business logic and orchestration.
-   **PromptBuilder**: Constructs deterministic prompts.
-   **EmbeddingService**: Generates vector embeddings.
-   **VectorStore**: Handles semantic search and indexing.
-   **LlmProvider**: Strategy abstraction for LLM integrations.
-   **OllamaProvider**: Concrete local LLM implementation.

------------------------------------------------------------------------

## ğŸ§  Key Features

### 1ï¸âƒ£ Local LLM Integration

Runs fully locally using Ollama (no API cost, no external dependency).

### 2ï¸âƒ£ Embeddings Support

Enables semantic search and RAG pipelines.

### 3ï¸âƒ£ Vector Store Abstraction

Swappable implementation (InMemory â†’ OpenSearch-ready).

### 4ï¸âƒ£ RAG Pipeline

-   Embed user query
-   Retrieve relevant documents
-   Inject context
-   Generate grounded answer

### 5ï¸âƒ£ Observability

Micrometer-based latency tracking for LLM calls.

------------------------------------------------------------------------

## ğŸ“¦ Project Structure

    ai-backend-platform
    â”œâ”€â”€ config
    â”œâ”€â”€ controller
    â”œâ”€â”€ service
    â”œâ”€â”€ llm
    â”œâ”€â”€ vector
    â”œâ”€â”€ model
    â”œâ”€â”€ exception
    â””â”€â”€ util

ai-backend-platform
â”œâ”€â”€ pom.xml
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # future: OpenSearch, monitoring
â”œâ”€â”€ .env                            # local environment variables
â””â”€â”€ src
    â”œâ”€â”€ main
    â”‚   â”œâ”€â”€ java
    â”‚   â”‚   â””â”€â”€ com/example/ai
    â”‚   â”‚       â”œâ”€â”€ AiApplication.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ config
    â”‚   â”‚       â”‚   â”œâ”€â”€ WebClientConfig.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ MetricsConfig.java
    â”‚   â”‚       â”‚   â””â”€â”€ AppProperties.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ controller
    â”‚   â”‚       â”‚   â”œâ”€â”€ ChatController.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ DocumentController.java
    â”‚   â”‚       â”‚   â””â”€â”€ HealthController.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ service
    â”‚   â”‚       â”‚   â”œâ”€â”€ ChatService.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ RagService.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ EmbeddingService.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ EvaluationService.java
    â”‚   â”‚       â”‚   â””â”€â”€ PromptBuilder.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ llm
    â”‚   â”‚       â”‚   â”œâ”€â”€ LlmProvider.java
    â”‚   â”‚       â”‚   â””â”€â”€ OllamaProvider.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ vector
    â”‚   â”‚       â”‚   â”œâ”€â”€ VectorStore.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ InMemoryVectorStore.java
    â”‚   â”‚       â”‚   â””â”€â”€ (future) OpenSearchVectorStore.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ model
    â”‚   â”‚       â”‚   â”œâ”€â”€ ChatRequest.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ StructuredResponse.java
    â”‚   â”‚       â”‚   â”œâ”€â”€ EmbeddingResponse.java
    â”‚   â”‚       â”‚   â””â”€â”€ DocumentRequest.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ exception
    â”‚   â”‚       â”‚   â”œâ”€â”€ GlobalExceptionHandler.java
    â”‚   â”‚       â”‚   â””â”€â”€ LlmException.java
    â”‚   â”‚       â”‚
    â”‚   â”‚       â””â”€â”€ util
    â”‚   â”‚           â”œâ”€â”€ JsonValidator.java
    â”‚   â”‚           â””â”€â”€ CosineSimilarity.java
    â”‚   â”‚
    â”‚   â””â”€â”€ resources
    â”‚       â”œâ”€â”€ application.yml
    â”‚       â”œâ”€â”€ application-dev.yml
    â”‚       â””â”€â”€ application-prod.yml
    â”‚
    â””â”€â”€ test
        â””â”€â”€ java
            â””â”€â”€ com/example/ai
                â”œâ”€â”€ service
                â”‚   â”œâ”€â”€ ChatServiceTest.java
                â”‚   â””â”€â”€ RagServiceTest.java
                â””â”€â”€ vector
                    â””â”€â”€ InMemoryVectorStoreTest.java
------------------------------------------------------------------------

## ğŸ”§ Setup Instructions

### Install Ollama

https://ollama.com

``` bash
ollama pull llama3
ollama run llama3
```

Default local endpoint:

http://localhost:11434

------------------------------------------------------------------------

## â–¶ Run Application

``` bash
mvn clean install
mvn spring-boot:run
```

App runs at:

http://localhost:8080

------------------------------------------------------------------------

## ğŸ¯ What This Project Demonstrates

-   Enterprise-ready AI backend architecture
-   Clean separation of concerns
-   Strategy-based LLM provider design
-   RAG implementation in Java
-   Production-oriented engineering thinking

------------------------------------------------------------------------

## ğŸ“„ License

MIT License
